1)This is Spark Training
This is Spark with Scala Training
Scala is Functional programming
Spark is lightning fast cluster computing framework

Answer:
package Nithin_Spark_Assignment

import org.apache.spark.{SparkConf, SparkContext}

object wordCount {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName(getClass.getName).setMaster("local")
    val sc = new SparkContext(conf)
    val fileRDD = sc.textFile("C:\\Users\\nithin\\Desktop\\Spark assignments\\WordCountFile.txt")
    val listRDD = fileRDD.flatMap(rec => rec.split(" "))
    val keyValueRDD = listRDD.map(ele => (ele,1))
    val groupRDD = keyValueRDD.reduceByKey(_+_)
    groupRDD.collect().foreach(println)
  }
}

==============================================================================================================================
2) Given two datasets,
a. User information (id, email, language, location)
1 email@test.com EN US
2 email@test2.com EN GB
3 email@test3.com FR FR

b. Transaction information (transaction-id, product-id, user-id, purchase-amount, item-
description)

1 1 1 300 item1
2 1 2 300 item2
3 1 2 300 item3
4 2 3 100 item4
5 1 3 300 item5
Use Spark-core (RDD) to find the number of unique locations in which each product
has been sold.

Answer:
package Nithin_Spark_Assignment

import org.apache.spark.{SparkConf, SparkContext}

object joinExample {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
      .setAppName(getClass.getName)
      .setMaster("local")
    val sc = new SparkContext(conf)
    val userRDD= sc.textFile("C:\\Users\\nithin\\Desktop\\Spark assignments\\userInformation.txt")
    val transRDD = sc.textFile("C:\\Users\\nithin\\Desktop\\Spark assignments\\transactional_Info.txt")
    val userkvRDD=userRDD.map(rec => {
      val feildArr=rec.split(" ")
      (feildArr(0),feildArr(3))
    })
    val transkvRDD=transRDD.map(rec => {
      val feildArr=rec.split(" ")
      (feildArr(2),feildArr(1))
    })
    val joinRDD= userkvRDD.join(transkvRDD)
    val uniRDD = joinRDD.map(t => (t._2._2,t._2._1)).distinct()
    uniRDD.collect().foreach(println)
  }
}
============================================================================================================================

3) Consider the file â€“ social_friends.csv and below is the description of columns in the file,
Column 1: User ID
Column 2: User Name
Column 3: Age of the User
Column 4: Number of Friends with that User
Write Spark-core (RDD) code to calculate the average number of friends based on their
age.

package Nithin_Spark_Assignment

import org.apache.spark.{SparkConf, SparkContext}

object social_friends {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName(getClass.getName).setMaster("local")
    val sc = new SparkContext(conf)
    val fileRDD =sc.textFile("C:\\Users\\nithin\\Desktop\\Spark assignments\\social_friends.csv")
    val selCRDD= fileRDD.map(rec => {
      val filedArr = rec.split(",")
      (filedArr(2).toInt,filedArr(3).toInt)
    })
    val groupRDD = selCRDD.groupByKey()
    val finalResult= groupRDD.map(t => (t._1,(t._2.sum/t._2.size)))
    finalResult.collect().foreach(println)
  }

}
